{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8d6351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it official u.s. president barack obama wants ...</td>\n",
       "      <td>syrian official obama climbed to the top of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnn usain bolt rounded off the world champion...</td>\n",
       "      <td>usain bolt wins third gold of world championsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kansas city missouri cnn the general services ...</td>\n",
       "      <td>the employee in agency kansas city office is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>los angeles cnn medical doctor in vancouver br...</td>\n",
       "      <td>new canadian doctor says she was part of team ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnn police arrested another teen thursday the...</td>\n",
       "      <td>another arrest made in gang rape outside calif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  it official u.s. president barack obama wants ...   \n",
       "1   cnn usain bolt rounded off the world champion...   \n",
       "2  kansas city missouri cnn the general services ...   \n",
       "3  los angeles cnn medical doctor in vancouver br...   \n",
       "4   cnn police arrested another teen thursday the...   \n",
       "\n",
       "                                               ctext  \n",
       "0  syrian official obama climbed to the top of th...  \n",
       "1  usain bolt wins third gold of world championsh...  \n",
       "2  the employee in agency kansas city office is a...  \n",
       "3  new canadian doctor says she was part of team ...  \n",
       "4  another arrest made in gang rape outside calif...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd958c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e1d01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c482b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1690cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_text_sents = [i.split() for i in data['text']]\n",
    "# model = Word2Vec(sentences = tokenized_text_sents, vector_size=400, window=5, min_count=1, workers=4)\n",
    "# model.save(\"custom_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0e8e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"custom_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6cbc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc):\n",
    "    words = model.wv.index_to_key\n",
    "    doc = [word for word in doc if word in words]\n",
    "    return np.mean(model.wv[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccbc6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def text_strip(row):\n",
    "    row = re.sub(\"(\\\\t)\", \" \", str(row)).lower()\n",
    "    row = re.sub(\"(\\\\r)\", \" \", str(row)).lower()\n",
    "    row = re.sub(\"(\\\\n)\", \" \", str(row)).lower()\n",
    "\n",
    "    # Remove _ if it occurs more than one time consecutively\n",
    "    row = re.sub(\"(__+)\", \" \", str(row)).lower()\n",
    "\n",
    "    # Remove - if it occurs more than one time consecutively\n",
    "    row = re.sub(\"(--+)\", \" \", str(row)).lower()\n",
    "\n",
    "    # Remove ~ if it occurs more than one time consecutively\n",
    "    row = re.sub(\"(~~+)\", \" \", str(row)).lower()\n",
    "\n",
    "    # Remove + if it occurs more than one time consecutively\n",
    "    row = re.sub(\"(\\+\\++)\", \" \", str(row)).lower()\n",
    "\n",
    "    # Remove . if it occurs more than one time consecutively\n",
    "    row = re.sub(\"(\\.\\.+)\", \" \", str(row)).lower()\n",
    "\n",
    "    # Remove the characters - <>()|&©ø\"',;?~*!\n",
    "    row = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", \" \", str(row)).lower()\n",
    "\n",
    "    # Remove mailto:\n",
    "    row = re.sub(\"(mailto:)\", \" \", str(row)).lower()\n",
    "\n",
    "    # Remove \\x9* in text\n",
    "    row = re.sub(r\"(\\\\x9\\d)\", \" \", str(row)).lower()\n",
    "\n",
    "    # Replace INC nums to INC_NUM\n",
    "    row = re.sub(\"([iI][nN][cC]\\d+)\", \"INC_NUM\", str(row)).lower()\n",
    "\n",
    "    # Replace CM# and CHG# to CM_NUM\n",
    "    row = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", \"CM_NUM\", str(row)).lower()\n",
    "\n",
    "    # Remove punctuations at the end of a word\n",
    "#     row = re.sub(\"(\\.\\s+)\", \" \", str(row)).lower()\n",
    "    row = re.sub(\"(\\-\\s+)\", \" \", str(row)).lower()\n",
    "    row = re.sub(\"(\\:\\s+)\", \" \", str(row)).lower()\n",
    "\n",
    "    # Replace any url to only the domain name\n",
    "    try:\n",
    "        url = re.search(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", str(row))\n",
    "        repl_url = url.group(3)\n",
    "        row = re.sub(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", repl_url, str(row))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    row = re.sub(\"(\\s+)\", \" \", str(row)).lower()\n",
    "\n",
    "    # Remove the single character hanging between any two spaces\n",
    "    row = re.sub(\"(\\s+.\\s+)\", \" \", str(row)).lower()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "655834ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      "  they are rightfully concerned that the film will sway some to become more receptive or even supportive of the idea of torturing prisoners.  indeed the response of sony pictures to the uproar over zero dark thirty tells you about what they really hope we will all do we encourage people to see the film before characterizing it\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    " \n",
    "def read_article(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    filedata = file.read()\n",
    "    filedata = text_strip(filedata)\n",
    "    filedata.replace('\\n\\n','')\n",
    "    filedata.replace('\\n','')\n",
    "    article = filedata.split(\".\")\n",
    "    sentences = []\n",
    "    for sentence in article:\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "def sentence_similarity_w2v(sent1, sent2, stopwords=None):\n",
    "    vector1 = document_vector(sent1)\n",
    "    vector2 = document_vector(sent2)\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity_w2v(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(file_name, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_article(file_name)\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)     \n",
    "\n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "    # Step 5 - Offcourse, output the summarize texr\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "\n",
    "# let's begin\n",
    "generate_summary( \"sample.txt\", top_n=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
