{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50794d86",
   "metadata": {},
   "source": [
    "#### Cedits: https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9b42764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73ad7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# WandB – Import the wandb library\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e66a15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "# Preparing for TPU usage\n",
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "# device = xm.xla_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a226f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login 288b8f7697201744f07825b641619180aa4f47dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1145374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = self.data.text\n",
    "        self.ctext = self.data.ctext\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f510ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids,labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "#         if _%10 == 0:\n",
    "#             wandb.log({\"Training Loss\": loss.item()})\n",
    "\n",
    "#         if _%500==0:\n",
    "#             print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # xm.optimizer_step(optimizer)\n",
    "        # xm.mark_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70528157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=512, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            if _%100==0:\n",
    "                print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeaeef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  It's official: U.S. President Barack Obama wan...   \n",
      "1  (CNN) -- Usain Bolt rounded off the world cham...   \n",
      "2  Kansas City, Missouri (CNN) -- The General Ser...   \n",
      "3  Los Angeles (CNN) -- A medical doctor in Vanco...   \n",
      "4  (CNN) -- Police arrested another teen Thursday...   \n",
      "\n",
      "                                               ctext  \n",
      "0  summarize: Syrian official: Obama climbed to t...  \n",
      "1  summarize: Usain Bolt wins third gold of world...  \n",
      "2  summarize: The employee in agency's Kansas Cit...  \n",
      "3  summarize: NEW: A Canadian doctor says she was...  \n",
      "4  summarize: Another arrest made in gang rape ou...  \n",
      "FULL Dataset: (10, 2)\n",
      "TRAIN Dataset: (8, 2)\n",
      "TEST Dataset: (2, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Fine-Tuning for the model on our dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\hugging_face\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# # WandB – Initialize a new run\n",
    "# wandb.init(project=\"transformers_tutorials_summarization\")\n",
    "\n",
    "# WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
    "# Defining some key variables that will be used later on in the training  \n",
    "# config = wandb.config          # Initialize config\n",
    "# config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
    "# config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
    "# config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n",
    "# config.VAL_EPOCHS = 1 \n",
    "# config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "# config.SEED = 42               # random seed (default: 42)\n",
    "# config.MAX_LEN = 512\n",
    "# config.SUMMARY_LEN = 512\n",
    "\n",
    "TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
    "VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
    "TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n",
    "VAL_EPOCHS = 1 \n",
    "LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "SEED = 42               # random seed (default: 42)\n",
    "MAX_LEN = 512\n",
    "SUMMARY_LEN = 512\n",
    "\n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(SEED) # pytorch random seed\n",
    "np.random.seed(SEED) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# tokenzier for encoding the text\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")#T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "\n",
    "# Importing and Pre-Processing the domain data\n",
    "# Selecting the needed columns only. \n",
    "# Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
    "df = pd.read_csv('data.csv',encoding='utf-8')\n",
    "df = df[['text','ctext']]\n",
    "df.ctext = 'summarize: ' + df.ctext\n",
    "print(df.head())\n",
    "df = df.head(10)\n",
    "\n",
    "\n",
    "# Creation of Dataset and Dataloader\n",
    "# Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state = SEED)\n",
    "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
    "\n",
    "\n",
    "# Creating the Training and Validation dataset for further creation of Dataloader\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "    'batch_size': TRAIN_BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "\n",
    "val_params = {\n",
    "    'batch_size': VALID_BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "\n",
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")#T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Log metrics with wandb\n",
    "# wandb.watch(model, log=\"all\")\n",
    "# Training loop\n",
    "print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "\n",
    "# Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "# Saving the dataframe as predictions.csv\n",
    "print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "for epoch in range(VAL_EPOCHS):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    final_df.to_csv('./models/predictions.csv')\n",
    "    print('Output Files generated for review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13daf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = './abstractive/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = '''(CNN) -- Loud music pumps through huge speakers, front row guests cheer and a parade of stunning models electrifies the gleaming catwalk -- welcome to Africa Fashion Week London.\n",
    "\n",
    "Currently in its third year, the glamorous event saw dozens of big names and up-and-coming designers from across the continent descending on the UK capital's hip district of Shoreditch to unveil their latest stylish creations.\n",
    "\n",
    "\"Our platform is about promoting emerging and established Africa-inspired designers,\" said Ronke Ademiluyi, founder of the event, held from August 1 to 3.\n",
    "\n",
    "\"The main thing is to bring attention to them, to showcase their creativity to the world so they get more global recognition for what they do and more appreciations for their brands as well.\"\n",
    "\n",
    "Vibrant colors\n",
    "\n",
    "The runways featured designs from countries such as Nigeria, Ghana, South Africa, Congo, Zambia, Zimbabwe, Botswana, Kenya and Morocco -- but also from the diaspora, including Britain and the Caribbean.\n",
    "\n",
    "In many ways, the event reaffirmed why Africa-inspired designs are fast catching the eye of the fashion world.\n",
    "\n",
    "Mixing current trends with traditional patterns, more than 60 designers graced the catwalk with a wide array of colorful creations -- everything from show-stopping evening gowns and modern urban casualwear to bold textured prints and chic accessory lines.\n",
    "\n",
    "Glorious colors in the desert: Darfur's fashionable women\n",
    "\n",
    "Ademiluyi says the continent's fashion today \"represents a fusion of contemporary and African designs\" awash with \"a lot of vibrant colors and tribal trends.\"\n",
    "\n",
    "Amongst those giving a modern twist to traditional styles is Nigerian designer Fashola Olayinka with her Lagos-based label \"MOOFA Designs.\" Her latest collection, \"Ashake\" is celebrating the \"very powerful and strong women\" who \"turn heads wherever they go to.\"\n",
    "\n",
    "\"That's basically what's the collection is about,\" says Olayinka, who started the label about four years ago. \"Women who are very feminine and sexy.\"\n",
    "\n",
    "Read this: Congo's designer dandies\n",
    "\n",
    "The young designer says that despite the existing challenges, such as frequent power cuts, it's very exciting being part of Nigeria's fashion scene right now.\n",
    "\n",
    "\"We work hard and we party hard, so it's been really fun and it's a growing process in Nigeria,\" she says. \"Nigerians like to dress up and a lot of people in Nigeria are now wearing their own fabrics.\"\n",
    "\n",
    "Profile boost\n",
    "\n",
    "But despite the growing interest in African designs, Ademiluyi says that many of the continent's promising talents still find it difficult to break into the mainstream international shows.\n",
    "\n",
    "She says that for many of them, the week is a chance to shine on the international stage.\n",
    "\n",
    "\"A lot of them are talented but they're struggling,\" she says. \"They don't have support from anywhere, so what we do is we support them -- it's an affordable platform for the designers to showcase their talents to the world.\"\n",
    "\n",
    "Read this: Taking African colors to America's Deep South\n",
    "\n",
    "South African fashion artist Steve Mandy agrees. He says that events like this help participants boost their profile both internationally and at home.\n",
    "\n",
    "\"You can meet some really important people here and I have already met some great people here that I think I'll do business with,\" says Durban-based Mandy, known for hand-painting on dresses and t-shirts.\n",
    "\n",
    "\"The other thing is the spin-off in terms of your own image, in terms of our audience back in South Africa -- the fact that you can say that you did African Fashion Week it promotes you and helps your product to gain integrity.\"\n",
    "\n",
    "Looking ahead, Ademiluyi says the goal is to hold the event twice a year and also establish a supply platform that would make the designers' creations more accessible to the world.\n",
    "\n",
    "\"For a lot of them, after Africa Fashion Week, that's it,\" she says. \"The clients don't know where to get their brand, so we hope to support the designers a lot more by opening up a distribution outlet.\"\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "answer = summarizer(sample)[0]['summary_text']\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fe5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
